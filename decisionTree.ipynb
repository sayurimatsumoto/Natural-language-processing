{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"assets/ufscar.png\" alt=\"Logo UFScar\" width=\"200\" align=\"left\"/><p><center>Universidade Federal de São Carlos (UFSCar)</center><br/><font size=\"4\"><center> Departamento de Computação, campus Sorocaba </center> </font>\n",
    "\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<font size = \"4\"><center><b> Grupo 5: Análise de sentimento de reviews na Amazon </b></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré processamento: \n",
    "Funções do arquivo pre_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_processing as pp\n",
    "import analysis as anl\n",
    "import pca\n",
    "# Categoria da base de dados a ser lida (do disco) e processada\n",
    "# [books, kitchen_&_housewares, electronics, dvd, all]\n",
    "category = 'books'\n",
    "\n",
    "# Se positivo, adiciona bigramas para reviews negativas\n",
    "# ex: ('not', 'good') equivale a uma única feature\n",
    "hNeg = True\n",
    "\n",
    "# Se positivo, adiciona substantivos\n",
    "noun = False\n",
    "\n",
    "# Executa ou não o chi-quadrado na base\n",
    "flagChi2 = True\n",
    "\n",
    "# Guarda as features ja processadas em X, a classe da amostra em Y e o vocabulario em vocabulary\n",
    "# hNeg e noun sao opcionais, por padrao hNeg=True, noun=False\n",
    "X, Y, vocabulary = pp.bow(category, hNeg, noun)\n",
    "\n",
    "print(\"Vocabulário possui \" + str(len(vocabulary)) + \" palavras!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separa os dados em treinamento e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X2, Y2 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "# Porcentagem de amostras destinadas à base de treino\n",
    "pTrain = 0.8\n",
    "\n",
    "# Executa o holdout e retorna os índices de treino e teste, mantendo a proporção original entre as classes\n",
    "train_index, test_index = anl.stratified_holdOut(Y, pTrain)\n",
    "\n",
    "# Guarda as amostras de treino e teste\n",
    "Xtrain, Xval = X2[train_index, :], X2[test_index, :]\n",
    "Ytrain, Yval = Y2[train_index], Y2[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleciona features com chi-quadrado (a partir dos dados de treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seta o valor de alpha para o chi-quadrado. \n",
    "# alpha e opcional, por padrão alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Chama a funcao para executar o chi-quadrado e retorna a nova base de dados reduzida\n",
    "# o novo vocabulario e os indices das features mantidas\n",
    "if (flagChi2):\n",
    "    Xtrain, new_vocabulary, index = pp.chi2(Xtrain, Ytrain, vocabulary, alpha)\n",
    "    # Seleciona apenas as features do indice retornado pelo chi-quadrado para a base de teste\n",
    "    Xval = Xval[:, index]\n",
    "\n",
    "#converte para outro tipo de matriz esparsa que facilita os cálculos do svm\n",
    "Xtrain = Xtrain.tolil() \n",
    "Xval = Xval.tolil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (flagChi2):\n",
    "    print(\"Número de features antes do chi-quadrado: \" + str(len(vocabulary)))\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Número de features após chi-quadrado: \" + str(len(new_vocabulary)))\n",
    "    # print(new_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As árvores de decisão são algoritmos de aprendizado supervisionados usados para tarefas de classificação e regressão. No presente projeto, será usado apenas para classificação. As árvores de decisão são atribuídas aos algoritmos de aprendizado baseados em informações que usam diferentes medidas de ganho de informação para aprendizado. A idéia principal das árvores de decisão é encontrar as features que contêm mais \"informações\" no atributo alvo e, em seguida, dividir o conjunto de dados ao longo dos valores desses atributos, de modo que as características da classe de destino dos sub dataset resultantes sejam tão puras quanto quanto possível. Dessa forma, o conjuto é dividido recursivamente, com forme o ganho de informação, até chegar nos nós folhas, que representam a classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganho de informação\n",
    "O ganho de informação é a medida usada para dividir o conjuto de dados. Há várias formas de cálcular esse ganho: Índice de Gini, Entropia, Qui-Quadrado, Índice de Ganho de Informação, Variância. Será utilizado a entropia, nesse projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropia\n",
    "A entropia de um conjunto de dados é usada para medir a impureza de um conjunto de dados. A fórmula da entropia: \n",
    "$$ Entropy(x) = - \\sum_{k \\in target} (p(x=k) * \\log_2 p(x=k)$$\n",
    "\n",
    "Onde p(x=k) é a probabidade de x ser igual a k. E k representa as classes do atributo alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "#Foi utilizado o cálculo da entropia com a biblioteca scipy\n",
    "def entropy(Y) :\n",
    "    entropia = st.entropy(Y)\n",
    "    return entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fórmula do ganho de informação:\n",
    "Para poder verificar qual das featuers divide mais precisamente o conjunto de dados, ou seja, permanece o conjunto de dados com a impureza mais baixa - entropia ou, em outras palavras, classifica melhor a classe do atributo alvo, é feito da seguinte forma: Para cada feature, é feita a divisão do conjunto de dados ao longo dos valores dessas features e, em seguida, calculamos a entropia do conjunto de dados depois de dividir os dados ao longo dos valores da feature. Isso nos dá a entropia restante depois que é divido o conjunto de dados ao longo dos valores dessa feature. Em seguida, subtraímos esse valor da entropia originalmente calculada do conjunto de dados para ver quanto essa divisão de features reduz a entropia original. O ganho de informação de uma feature pode ser cálculada:\n",
    "\n",
    "$$ InfoGain(feature_d)=Entropy(D)−Entropy(feature_d)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fórmula para cálcular a Entropia por feature:\n",
    "\n",
    "$$ InfoGain(feature_d, D) = Entropy(D) - \\sum_{k \\in target} \\frac{| feature_d=k|}{|D|} * Entropia(feature_d=k) $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoGain(X, Y, split_atribute) :\n",
    "    totalEntropy = entropy(Y) # entropia(D)\n",
    "    valores, count = np.unique(X[:,split_atribute], return_counts = True)\n",
    "    entropyPond = 0 # entropia poderada, entropia da feature split_atribute\n",
    "    for i in range(len(valores)) : \n",
    "        \n",
    "        myatt = entropy(np.where(X[:,split_atribute]==valores[i])[0]) # Entropia(X[feature_d==k])\n",
    "        entropyPond += np.sum([(count[i]/np.sum(count)*myatt)]) # somatório de qtd(feature_d==k)/qtd(total) * Entropia(X[feature_d==k])\n",
    "      \n",
    "    informationGain =  totalEntropy - entropyPond\n",
    "    \n",
    "    return informationGain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo ID3\n",
    "O algoritmo ID3 é um algoritmo popular para o crescimento de árvores de decisão, publicado por Ross Quinlan em 1986. Além do algoritmo ID3, há também outros algoritmos populares como o C4.5, o C5.0 e o CART. Será usado o ID3, por possuir a implementação mais simples.\n",
    "\n",
    "O primeiro passo é defir os critérios de parada e como tratar cada um deles:\n",
    "\n",
    "1 - Se todas as linhas do atributo alvo possuirem o mesmo valor:\n",
    "    - Retorna a classe do atributo de alvo.\n",
    "    \n",
    "2 - O conjunto de dados não pode mais ser dividido, pois não há mais daos:\n",
    "    - Será atribuido ao nó folha o valor do atributo alvo que ocorre com mais frequência no nó superior (pai). \n",
    "    \n",
    "3 - O conjunto de dados não pode mais ser dividido, pois não tem mais features:\n",
    "    - Será atribuido ao nó folha do atributo alvo o nó pai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de definidos os critérios de parada, o algoritmo ID3 funciona de seguinte maneira:\n",
    "- Escolher a feature que tem mais ganho de informação\n",
    "- Para cada valor diferente na feature, adicione um nó sobre a raiz\n",
    "- Para cada novo nó criar uma sub árvore chamando recursivamente o ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(X, originalX, Y, originalY, features, parentNodeClasse = None) :\n",
    "    # 1 - Se todas as linhas do atributo alvo possuirem o mesmo valor:\n",
    "    if (len(np.unique(Y)) <= 1) :\n",
    "        return np.unique(Y)[0] #retorna esse valor\n",
    "    \n",
    "    if (len(X)==0) : #O conjunto de dados estiver vazio:\n",
    "        #o valor do atributo alvo que ocorre com mais frequência no nó superior (pai).\n",
    "        return np.unique(originalY)[np.argmax(np.unique(originalY, return_counts = True))]\n",
    "    \n",
    "    if (len(features)==0) : # Se não houver mais feares\n",
    "        return parentNodeClasse #retornar o atributo alvo do nó pai\n",
    "    else :\n",
    "        # classe do no pai, valor default -> classe majoritária em Y\n",
    "        parentNodeClasse = np.unique(Y)[np.argmax(np.unique(Y,return_counts = True)[1])]\n",
    "        bestGain = -330 #inicializando melhor ganho, -330, valor arbitrário negativo suficientemente pequeno\n",
    "        #Procurar feature que o ferece melhor ganho\n",
    "        bestFeature = features[0] #inicializando melhor feature\n",
    "        for f in features :\n",
    "            gain_new = infoGain(X,Y, f)\n",
    "            if(gain_new > bestGain) : \n",
    "                bestGain = gain_new\n",
    "                bestFeature = f\n",
    "        #Estrutura da árvore        \n",
    "        tree = {bestFeature:{}}\n",
    "        \n",
    "        #Tirar feature com melhor ganho do conjunto de feature\n",
    "        #features.delete(bestFeature)\n",
    "        features = [i for i in features if i != bestFeature]\n",
    "        \n",
    "        #Para cada valor na coluna da feature com melhor ganho\n",
    "        for value in np.unique(X[:,bestFeature]) :\n",
    "            value = value #evita um bug\n",
    "            # dividindo os subconjunto\n",
    "            subX_indes = np.where(X[:,bestFeature] == value)[0]\n",
    "            subX = X[subX_indes]\n",
    "            #subX e subY = seubConjunto onde feature escolhida possui o valor\n",
    "            subY = Y[np.where(X[:,bestFeature] == value)]\n",
    "            \n",
    "            #chamada recursiva para ID3 -> crescer a árvore\n",
    "            subTree = ID3(subX, X, subY, Y, features, parentNodeClasse)\n",
    "            \n",
    "            #colocar na estrutura da árvore \n",
    "            tree[bestFeature][value] = subTree\n",
    "            \n",
    "        return(tree) \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição:\n",
    "A predição ocorre de meneira recursiva, percorrendo a àrvore e verificando se chegou em um nó folha ou se ainda está em uma sub árvore.\n",
    "\n",
    "#### Observação Importante\n",
    "Desde que o modelo da árvore foi treinado e, em seguida, é mostrado ao modelo uma amostra que o modelo nunca havia visto, pode acontecer que os valores das features destas amostra não existam no modelo da árvore, porque não são as instâncias de treinamento tiveram esse valor para essa feature em específico. Isso causa uma exception e precisa ser tratada. Como este é um algoritmo simplificado, tratamos esse erro, retornando uma classe default, que nesse projeto, não é grave a predição incorreta. Como a maioria dos reviews na Amazon são positivos, a classe default escolhida foi a 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, tree,default=1) :\n",
    "    features = np.arange(Xtrain.shape[1])\n",
    "    for f in features : #andar pelas features\n",
    "        if f in list(tree.keys()) : #verificar se esta feature corresponde ao nó percorrido. No caso da primeira rodada, tem que ser a feature da raiz\n",
    "            try : # referente a observação feita\n",
    "                classe = tree[f][row[f]] #caso não tenha na árvore\n",
    "            except:\n",
    "                return default    # retorne o valor padrão\n",
    "            \n",
    "            classe = tree[f][row[f]] #caso tenha na árvore\n",
    "            \n",
    "            #verificar se é folha ou não\n",
    "            if isinstance(classe,dict):\n",
    "                return predict(row,classe) # se não for folha, chamar recursivamente a predição\n",
    "            else:\n",
    "                return classe # se for folha  retorna a classe\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test, tree) :\n",
    "    result = []\n",
    "    for row in range(test.shape[0]) :\n",
    "        t = test[row]\n",
    "        result.append(predict(t, tree))\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "features_index = np.arange(Xtrain.shape[1])\n",
    "\n",
    "#treinamento\n",
    "tree = ID3(Xtrain.toarray(), Xtrain.toarray() ,Ytrain, Ytrain, features_index)\n",
    "\n",
    "#pprint(tree) #se quiser imprimir a árvore\n",
    "result = test(Xval.toarray(), tree)\n",
    "\n",
    "acuracia = np.sum(result==Yval)/len(Yval)\n",
    "print(acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação com K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k_folds as kf\n",
    "\n",
    "#Pega todos os tipos de classes \n",
    "classes = classes = np.unique(Y)\n",
    "\n",
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X3, Y3 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "# separa os dados em k folds\n",
    "nFolds = 5\n",
    "folds = kf.stratified_kfolds(Y3, nFolds, classes)\n",
    "\n",
    "k = 1\n",
    "resultados=[] # cria uma lista vazia para guardar os resultados obtidos em cada fold\n",
    "\n",
    "for train_index, test_index in folds:\n",
    "\n",
    "    print('\\n-----------\\n%d-fold: \\n-----------\\n' % (k) )\n",
    "\n",
    "    # se train_index ou test_index forem vazios, interrompe o laco de repeticao\n",
    "    if len(train_index)==0 or len(test_index)==0: \n",
    "        print('\\tErro: o vetor com os indices de treinamento ou o vetor com os indices de teste esta vazio')      \n",
    "        break\n",
    "        \n",
    "    Xtrain, Xval = X3[train_index, :], X3[test_index, :];\n",
    "    Ytrain, Yval= Y3[train_index], Y3[test_index];\n",
    "\n",
    "    if (flagChi2):\n",
    "        Xtrain, new_vocabulary, index = pp.chi2(Xtrain, Ytrain, vocabulary)\n",
    "        Xval = Xval[:, index]\n",
    "    \n",
    "    #Converte matrizes esparsas para np arrays, para os cálculos da rede neural\n",
    "    Xtrain = Xtrain.toarray()\n",
    "    Xval = Xval.toarray()\n",
    "    \n",
    "    features_index = np.arange(Xtrain.shape[1])\n",
    "    tree = ID3(Xtrain, Xtrain ,Ytrain, Ytrain, features_index)\n",
    "\n",
    "    #pprint(tree)\n",
    "    Ypred = test(Xval, tree)\n",
    "    auxResults = anl.relatorioDesempenho(Yval, Ypred, classes, imprimeRelatorio=True)\n",
    "\n",
    "    # adiciona os resultados do fold atual na lista de resultados\n",
    "    resultados.append( auxResults )\n",
    "    \n",
    "    k = k + 1\n",
    "    \n",
    "kf.mediaFolds( resultados, classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
