{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"assets/ufscar.png\" alt=\"Logo UFScar\" width=\"200\" align=\"left\"/><p><center>Universidade Federal de São Carlos (UFSCar)</center><br/><font size=\"4\"><center> Departamento de Computação, campus Sorocaba </center> </font>\n",
    "\n",
    "\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<font size = \"4\"><center><b> Grupo 5: Análise de sentimento de reviews na Amazon </b></center></font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_processing as pp\n",
    "import analysis as anl\n",
    "import pca\n",
    "\n",
    "# Categoria da base de dados a ser lida (do disco) e processada\n",
    "# [books, kitchen_&_housewares, electronics, dvd, all]\n",
    "category = 'books'\n",
    "\n",
    "# Se positivo, adiciona bigramas para reviews negativas\n",
    "# ex: ('not', 'good') equivale a uma única feature\n",
    "hNeg = True\n",
    "\n",
    "# Se positivo, adiciona substantivos\n",
    "noun = False\n",
    "\n",
    "# Executa ou não o chi-quadrado na base\n",
    "flagChi2 = True\n",
    "\n",
    "# Guarda as features ja processadas em X, a classe da amostra em Y e o vocabulario em vocabulary\n",
    "# hNeg e noun sao opcionais, por padrao hNeg=True, noun=False\n",
    "X, Y, vocabulary = pp.bow(category, hNeg, noun)\n",
    "\n",
    "print(\"Vocabulário possui \" + str(len(vocabulary)) + \" palavras!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separa os dados em treinamento e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X2, Y2 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "# Porcentagem de amostras destinadas a base de treino\n",
    "pTrain = 0.8\n",
    "\n",
    "# Executa o holdout e retorna os indices de treino e teste, mantendo a proporcao original entre as classes\n",
    "train_index, test_index = anl.stratified_holdOut(Y, pTrain)\n",
    "\n",
    "# Guarda as amostras de treino e teste\n",
    "Xtrain, Xval = X2[train_index, :], X2[test_index, :]\n",
    "Ytrain, Yval = Y2[train_index], Y2[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleciona features com chi-quadrado (a partir dos dados de treinamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seta o valor de alpha para o chi-quadrado. \n",
    "# alpha e opcional, por padrão alpha = 0.05\n",
    "alpha = 0.05\n",
    "\n",
    "# Chama a funcao para executar o chi-quadrado e retorna a nova base de dados reduzida\n",
    "# o novo vocabulario e os indices das features mantidas\n",
    "if (flagChi2):\n",
    "    Xtrain, new_vocabulary, index = pp.chi2(Xtrain, Ytrain, vocabulary, alpha)\n",
    "    # Seleciona apenas as features do indice retornado pelo chi-quadrado para a base de teste\n",
    "    Xval = Xval[:, index]\n",
    "\n",
    "# Converte matrizes esparsas para np arrays, para os cálculos da rede neural\n",
    "Xtrain = Xtrain.toarray()\n",
    "Xval = Xval.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (flagChi2):\n",
    "    print(\"Número de features antes do chi-quadrado: \" + str(len(vocabulary)))\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Número de features após chi-quadrado: \" + str(len(new_vocabulary)))\n",
    "    # print(new_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função auxiliar para calcular a Função Sigmóide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calcula a funcao sigmoidal  \n",
    "    \"\"\"\n",
    "    # Verifica se z e inteiro\n",
    "    if isinstance(z, int):\n",
    "        g = 0\n",
    "    \n",
    "    # se z não é um inteiro, significa que é um array e inicia com a dimensão do array\n",
    "    else:\n",
    "        g = np.zeros( z.shape );\n",
    "\n",
    "    # Calculo vetorial da funcao sigmoidal\n",
    "    g = 1/(1 + np.exp(-z))\n",
    "  \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função auxiliar para calcular a derivada da Função Sigmóide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "    g = sigmoid(z)*(1 - sigmoid(z))    \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função auxiliar para calcular a Função Custo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def funcaoCusto_backp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, flagreg = False,lambda_reg = 1):\n",
    "    \n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # Custo\n",
    "    J = 0\n",
    "    \n",
    "    # Theta das redes neurais\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    # Constante para evitar erro de precisao numerica\n",
    "    eps = 1e-15\n",
    "\n",
    "    # Seta os rotulos como verdadeiro apenas para a classe da amostra\n",
    "    rotulos = np.zeros((len(y), num_labels), dtype=int) \n",
    "    for i in range(len(rotulos)):\n",
    "        rotulos[i][y[i]] = 1\n",
    "\n",
    "    # Calcula os nos da camada oculta\n",
    "    a1 = np.insert(X, 0, np.ones(m, dtype=int), axis=1)\n",
    "    z2 = np.dot(a1, Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.insert(a2, 0, np.ones(a2.shape[0], dtype=int), axis=1)\n",
    "    z3 = np.dot(a2, Theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    \n",
    "    delta3 = a3 - rotulos\n",
    "    delta2 = np.dot(delta3, Theta2[:, 1:]) * sigmoidGradient(z2)\n",
    "\n",
    "    if flagreg == True :\n",
    "        reg = (lambda_reg/(2*m))*(np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2)) \n",
    "\n",
    "        J += reg\n",
    "\n",
    "        Theta1_grad[:, 0] = (np.dot(delta2.T, a1)[:, 0]/m) \n",
    "        Theta1_grad[:, 1:] = (np.dot(delta2.T, a1)[:, 1:]/m) + (lambda_reg/m)*Theta1[:, 1:]\n",
    "\n",
    "        Theta2_grad[:, 0] = (np.dot(delta3.T, a2)[:, 0]/m) \n",
    "        Theta2_grad[:, 1:] = (np.dot(delta3.T, a2)[:, 1:]/m) + (lambda_reg/m)*Theta2[:, 1:]\n",
    "    \n",
    "    else:    \n",
    "        Theta1_grad = np.dot(delta2.T, a1)/m\n",
    "        Theta2_grad = np.dot(delta3.T, a2)/m\n",
    "     \n",
    "    J = np.sum(-rotulos*np.log(a3+eps) - (1 - rotulos)*np.log(1 - a3+eps))/m\n",
    "\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função auxiliar para predizer um conjunto de amostras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao(Theta1, Theta2, Xval, Yval):\n",
    "\n",
    "    m = Xval.shape[0] # número de amostras\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    a1 = np.hstack( [np.ones([m,1]), Xval] )\n",
    "    h1 = sigmoid( np.dot(a1, Theta1.T) )\n",
    "\n",
    "    a2 = np.hstack( [np.ones([m,1]), h1] ) \n",
    "    h2 = sigmoid( np.dot(a2,Theta2.T) )\n",
    "    \n",
    "    Ypred = np.argmax(h2,axis=1)\n",
    "    \n",
    "    acuracia = np.sum(Ypred==Yval)/len(Yval)   \n",
    "        \n",
    "    return Ypred, acuracia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch: \n",
    "Itera sobre uma lista de possíveis valores para lambda_reg e retorna o melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(X, Y ,Xval, Yval):\n",
    "    \n",
    "    lambda_rnn = [0.01, 0.1, 10, 100]\n",
    "    \n",
    "    bestLambda = lambda_rnn[0]\n",
    "    bestAcc = 0\n",
    "    \n",
    "    for l in lambda_rnn:\n",
    "        # Treina e retorna os thetas\n",
    "        Theta1, Theta2 = redes_neurais(Xtrain, Ytrain, l)\n",
    "        \n",
    "        # Realiza a predição e guarda os resultados e a acuracia\n",
    "        Ypred, acuracia = predicao(Theta1, Theta2, Xval, Yval)\n",
    "        if bestAcc < acuracia :\n",
    "            bestLambda = l\n",
    "            bestAcc = acuracia\n",
    "        \n",
    "    return bestLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento RNN:\n",
    "Minimiza o custo da rede neural, retornando vetores de thetas ótimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redes_neurais (Xtrain, Ytrain, lambda_rnn, maxIter = 500):\n",
    "\n",
    "    input_layer_size  = Xtrain.shape[1]  # 20x20 dimensao das imagens de entrada\n",
    "    hidden_layer_size = int((input_layer_size + 2)*2/3)   \n",
    "    num_labels = 2          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # carregando os pesos da camada 1 com valores aleatorios\n",
    "    Theta1 =  np.random.RandomState(10).rand(hidden_layer_size, 1 + input_layer_size) * 2 * epsilon_init - epsilon_init\n",
    "    \n",
    "    # carregando os pesos da camada 2 com valores aleatorios\n",
    "    Theta2 = np.random.RandomState(10).rand(num_labels, 1 + hidden_layer_size) * 2 * epsilon_init - epsilon_init\n",
    "    \n",
    "    # concatena os pesos em um único vetor\n",
    "    initial_rna_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "    \n",
    "    #lamba_reg = 1\n",
    "    \n",
    "    # Minimiza a funcao de custo\n",
    "    result = scipy.optimize.minimize(fun=funcaoCusto_backp, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, Xtrain, Ytrain,True,lambda_rnn),  \n",
    "                    method='TNC', jac=True, options={'maxiter': maxIter})\n",
    "    \n",
    "    # Coleta os pesos retornados pela função de minimização\n",
    "    nn_params = result.x\n",
    "    \n",
    "    # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "    \n",
    "    return Theta1, Theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predição da base de teste do Holdout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_rnn = gridSearch(Xtrain, Ytrain, Xval, Yval)\n",
    "Theta1, Theta2 = redes_neurais(Xtrain, Ytrain, lambda_rnn, maxIter = 500)\n",
    "\n",
    "Ypred, acuracia = predicao(Theta1, Theta2, Xval, Yval)\n",
    "print(\"Acurácia é \"+ str(acuracia*100))\n",
    "\n",
    "classes = np.unique(Y)\n",
    "auxResults = anl.relatorioDesempenho(Yval, Ypred, classes, imprimeRelatorio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva de Aprendizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curva_aprendizado(X, Y, Xval, Yval, lambda_rnn, num_iteracoes = 0):\n",
    "   \n",
    "    \"\"\"\n",
    "    Funcao usada gerar a curva de aprendizado.\n",
    "  \n",
    "    Parametros\n",
    "    ----------\n",
    "  \n",
    "    X : matriz com os dados de treinamento\n",
    "  \n",
    "    Y : vetor com as classes dos dados de treinamento\n",
    "  \n",
    "    Xval : matriz com os dados de validação\n",
    "  \n",
    "    Yval : vetor com as classes dos dados de validação\n",
    "    \n",
    "    lambda_rnn : fator de regularização da função de custo\n",
    "    \n",
    "    num_iteracoes : escalar indicando a quantidade de iterações da curva de aprendizado\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # Define a quantidade de iteracoes, por padrao, itera por cada amostra da base de treino\n",
    "    if (num_iteracoes <= 0 or num_iteracoes > len (Y)):\n",
    "        num_iteracoes = len (Y)\n",
    "    \n",
    "    # inicializa as listas que guardarao a performance no treinamento e na validacao\n",
    "    perf_train = []\n",
    "    perf_val = []\n",
    "\n",
    "    # Guarda a quantidade de classes da base\n",
    "    classes = np.unique(Y)\n",
    "    \n",
    "    # Itera e executa a Rede Neural com i amostras da base de treino para cada iteracao\n",
    "    for i in range(10, len(Y), int (len(Y)/num_iteracoes)):\n",
    "        # Executa o treinamento e retorna os thetas\n",
    "        Theta1, Theta2 = redes_neurais(X[:i], Y[:i], lambda_rnn)\n",
    "\n",
    "        # Realiza a predição usando a base de treino e retorna a acuracia e as classes preditas\n",
    "        Ypred, acuracia = predicao(Theta1, Theta2, X[:i], Y[:i])\n",
    "        \n",
    "        # Guarda a acuracia da iteracao usando a base de treino como teste\n",
    "        perf_train.append(acuracia)\n",
    "\n",
    "        # Realiza a predição usando a base de teste e retorna a acuracia e as classes preditas\n",
    "        Ypred, acuracia = predicao(Theta1, Theta2, Xval, Yval)\n",
    "        \n",
    "        # Guarda a acuracia da iteracao usando a base de validacao como teste\n",
    "        perf_val.append(acuracia)\n",
    "\n",
    "\n",
    "    ##################################################################################\n",
    "       \n",
    "    # Define o tamanho da figura \n",
    "    plt.figure(figsize=(20,12))\n",
    "\n",
    "    # Plota os dados\n",
    "    plt.plot(perf_train, color='blue', linestyle='-', linewidth=1.5, label='Treino') \n",
    "    plt.plot(perf_val, color='red', linestyle='-', linewidth=1.5, label='Validação')\n",
    "\n",
    "    # Define os nomes do eixo x e do eixo y\n",
    "    plt.xlabel(r'# Qtd. de dados de treinamento',fontsize='x-large') \n",
    "    plt.ylabel(r'Acuracia',fontsize='x-large') \n",
    "\n",
    "    # Define o título do gráfico\n",
    "    plt.title(r'Curva de aprendizado', fontsize='x-large')\n",
    "\n",
    "    # Acrescenta um grid no gráfico\n",
    "    plt.grid(axis='both')\n",
    "\n",
    "    # Plota a legenda\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_rnn = fator de regularizacao da rede neural\n",
    "# num_iteracoes = numero de iteracoes da curva de aprendizado, por padrao e igual ao numero de amostras da base\n",
    "lambda_rnn = 10\n",
    "curva_aprendizado(Xtrain, Ytrain, Xval, Yval, lambda_rnn, num_iteracoes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k_folds as kf\n",
    "\n",
    "#Pega todos os tipos de classes \n",
    "classes = classes = np.unique(Y)\n",
    "\n",
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X3, Y3 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "# separa os dados em k folds\n",
    "nFolds = 5\n",
    "folds = kf.stratified_kfolds(Y3, nFolds, classes)\n",
    "\n",
    "k = 1\n",
    "\n",
    "# cria uma lista vazia para guardar os resultados obtidos em cada fold\n",
    "resultados=[] \n",
    "\n",
    "for train_index, test_index in folds:\n",
    "\n",
    "    print('\\n-----------\\n%d-fold: \\n-----------\\n' % (k) )\n",
    "\n",
    "    # se train_index ou test_index forem vazios, interrompe o laco de repeticao\n",
    "    if len(train_index)==0 or len(test_index)==0: \n",
    "        print('\\tErro: o vetor com os indices de treinamento ou o vetor com os indices de teste esta vazio')      \n",
    "        break\n",
    "        \n",
    "    # Guarda as bases de treino e teste baseado nos índices de cada fold\n",
    "    Xtrain, Xval = X3[train_index, :], X3[test_index, :];\n",
    "    Ytrain, Yval= Y3[train_index], Y3[test_index];\n",
    "\n",
    "    if (flagChi2):\n",
    "        # Executa o chi-quadrado na base do fold atual\n",
    "        Xtrain, new_vocabulary, index = pp.chi2(Xtrain, Ytrain, vocabulary)\n",
    "        Xval = Xval[:, index]\n",
    "    \n",
    "    #Converte matrizes esparsas para np arrays, para os cálculos da rede neural\n",
    "    Xtrain = Xtrain.toarray()\n",
    "    Xval = Xval.toarray()\n",
    "    \n",
    "    # Executa o gridSearch e retorna o melhor fator de regularizacao \n",
    "    lambda_rnn = gridSearch(Xtrain, Ytrain, Xval, Yval)\n",
    "    \n",
    "    # Executa o treinamento e guarda os thetas\n",
    "    Theta1, Theta2 = redes_neurais(Xtrain, Ytrain, lambda_rnn)\n",
    "    \n",
    "    # Faz a predicao\n",
    "    Ypred, acuracia = predicao(Theta1, Theta2, Xval, Yval)\n",
    "    \n",
    "    print(\"Melhor lambda\")\n",
    "    print(lambda_rnn)\n",
    "    \n",
    "    # Relatorio do fold\n",
    "    auxResults = anl.relatorioDesempenho(Yval, Ypred, classes, imprimeRelatorio=True)\n",
    "\n",
    "    # adiciona os resultados do fold atual na lista de resultados\n",
    "    resultados.append( auxResults )\n",
    "    \n",
    "    k = k + 1\n",
    "\n",
    "#Calcula a media dos resultados do k-fold\n",
    "kf.mediaFolds( resultados, classes )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
